{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.image import ContainerImage, Image\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.core import Environment, Experiment\n",
    "from azureml.core import Datastore, Dataset\n",
    "from azureml.core import Keyvault\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "keyvault = ws.get_default_keyvault()\n",
    "\n",
    "tenant_id = keyvault.get_secret(\"tenantid\")\n",
    "client_id = keyvault.get_secret(\"clientid\")\n",
    "client_secret = keyvault.get_secret(\"clientsecret\")\n",
    "sablobsampleaccountkey = keyvault.get_secret(\"sablobsampleaccountkey\")\n",
    "'''\n",
    "sp = ServicePrincipalAuthentication(tenant_id=tenant_id, # tenantID\n",
    "                                    service_principal_id=client_id, # clientId\n",
    "                                    service_principal_password=client_secret) # clientSecret\n",
    "\n",
    "subscription_id = '7e48a1e8-8d3e-4e00-8bc0-098c43f5ace7'\n",
    "\n",
    "# Azure Machine Learning resource group NOT the managed resource group\n",
    "resource_group = 'rg-mlops-demo-dev' \n",
    "\n",
    "#Azure Machine Learning workspace name, NOT Azure Databricks workspace\n",
    "workspace_name = 'ws-demo' \n",
    "ws = Workspace.get(name=workspace_name,\n",
    "                   auth=sp,\n",
    "                   subscription_id=subscription_id\n",
    "                  ,resource_group=resource_group)\n",
    "#ws.get_details()\n",
    "dstore = ws.get_default_datastore()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model picked: diabetes \n",
      "Model Description: None \n",
      "Model Version: 5\n"
     ]
    }
   ],
   "source": [
    "model_name = \"diabetes\"\n",
    "model_version = 5\n",
    "\n",
    "\n",
    "model_list = Model.list(workspace=ws)\n",
    "model = [m for m in model_list if m.version == model_version and m.name == model_name]\n",
    "if len(model) > 0:\n",
    "    model = model[0]\n",
    "else:\n",
    "    print(\"No model found\")\n",
    "    \n",
    "print(\n",
    "    \"Model picked: {} \\nModel Description: {} \\nModel Version: {}\".format(\n",
    "        model.name, model.description, model.version\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new compute target...\n",
      "InProgress....\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Resizing', 'allocationStateTransitionTime': '2021-11-17T04:41:01.968000+00:00', 'errors': None, 'creationTime': '2021-11-17T04:41:01.574004+00:00', 'modifiedTime': '2021-11-17T04:41:17.133001+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = \"cpu-cluster\"\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size =  \"STANDARD_D2_V2\"\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "#os.chdir(\"./code/scoring\")\n",
    "env = Environment.from_conda_specification(name = \"diabetesenv\",\n",
    "                                             file_path = \"./code/scoring/conda_dependencies.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datastore\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "#datastore_name=\"dstore_diabetes\"\n",
    "#adls_datastore = ws.datastores[datastore_name]\n",
    "\n",
    "datastore_name=\"dstore_blob_diabetes\"\n",
    "\n",
    "if datastore_name in ws.datastores:\n",
    "    blob_datastore = ws.datastores[datastore_name]\n",
    "else:\n",
    "    print(\"Creating datastore\")\n",
    "    blob_datastore = Datastore.register_azure_blob_container(\n",
    "       workspace=ws,\n",
    "       datastore_name=datastore_name,\n",
    "       container_name=\"diabetes\", # subscription id of ADLS account\n",
    "       account_name=\"sablobsample\", # ADLS account name\n",
    "       account_key = sablobsampleaccountkey)\n",
    "    \n",
    "#path_on_datastore = adls_datastore.path('diabetes_inference')\n",
    "input_diabetes_ds = Dataset.File.from_files(path=(blob_datastore, \"/diabetes_inference\"), validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_param = PipelineParameter(name=\"diabetes_param\", default_value=input_diabetes_ds)\n",
    "input_diabetes_ds_consumption = DatasetConsumptionConfig(\"diabetes_param_config\", pipeline_param).as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"diabetes_inference_results\", datastore=blob_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=\"./code\",\n",
    "    entry_script=\"scoring/score_with_parallelrun.py\",\n",
    "    mini_batch_size=PipelineParameter(name=\"batch_size_param\", default_value=\"5\"),\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"mnist_outputs.txt\",\n",
    "    environment=env,\n",
    "    compute_target=compute_target,\n",
    "    process_count_per_node=PipelineParameter(name=\"process_count_param\", default_value=2),\n",
    "    node_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"predict-diabetes\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[ input_diabetes_ds_consumption ],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_on_datastore = mnist_data.path('mnist/0.png')\n",
    "new_diabetets_ds = Dataset.File.from_files(path=(blob_datastore, \"/diabetes_inference1\"), validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step predict-diabetes [1012e7eb][685420f5-8cb1-40e1-97e2-9ee772d0ba92], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 8c40d80c-02ef-487e-b184-984920ea6df0\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/8c40d80c-02ef-487e-b184-984920ea6df0?wsid=/subscriptions/7e48a1e8-8d3e-4e00-8bc0-098c43f5ace7/resourcegroups/rg-mlops-demo-dev/workspaces/ws-demo&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "pipeline_run_2 = Experiment(ws, 'diabetes_pred').submit(pipeline, \n",
    "                                   pipeline_parameters={\"diabetes_param\": new_diabetets_ds, \n",
    "                                                        \"batch_size_param\": \"1\",\n",
    "                                                        \"process_count_param\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline1 = pipeline_run_2.publish_pipeline(\n",
    "     name=\"Diabetes inference Pipeline\",\n",
    "     description=\"Diabetes inference Pipeline\",\n",
    "     version=\"1.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
